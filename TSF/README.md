# TSF Structure

The TSF-related additions, such as the Trustable Graph and tooling extensions, are primarily organized within the TSF folder:
- the items of the Trustable Graph are in `TSF/trustable` 
- the building blocks of the documentation, which is built with the S-CORE docs-as-code tool, is placed in `TSF/docs` 
- the report generated by [trudag](https://codethinklabs.gitlab.io/trustable/trustable/trudag/usage.html) is placed in `TSF/docs/generated`. This can either be produced as part of the CI pipeline or manually using the shell script `./TSF/scripts/generate_documentation.sh_`. It is strongly recommended that the TSF/docs folder is included in the .gitignore file
- the utility scripts are documented in the `TSF/scripts` folder

The TSF graph (including links, nodes and their hashes) is saved in the `.dotstop.dot` file and the trudag extensions including CPP test references are stored in the `.dotstop_extensions` folder since these locations are required by the trudag tool.

# Forking the repository

In order to fork this repository or set up any repository where the TSF documentation in this repository is to be included, the following settings have to be configured on GitHub. In addition to the below settings, make sure to also configure appropriate branch protection rules. 

- In `Settings` > `General` >`Features`:
    - Enable `Issues`

- In `Settings` > `Code and automation` > `Actions` > `General` > `Workflow Permissions`:
    - Make sure that only the following settings are enabled:
        - `Allow all actions and reusable workflows`
        - `Require approval for first-time contributors`
        - `Read repository contents and packages permissions`

- In `Settings` > `Code and automation` > `Pages`:
    - Under `Source`, select `GitHub Actions`

- In `Settings` > `Security` > `Advanced Security`:
    - Make sure that only the following settings are enabled:
        - `Dependency graph`
        - `Dependabot alerts`

- In `Actions tab`:
    - Click `I understand my workflows, go ahead and enable them`
    - In the left side menu, click `Show more workflows...` and enable any workflows which are labelled as `Disabled`

# Release management 

The releases process of this repository shall conform to the [release management plan of Eclipse S-CORE](https://github.com/eclipse-score/score/blob/668bae4d1a704565983d34b8447d5a035696299a/docs/platform_management_plan/release_management.rst#id11). Most notably, the release tags shall follow semantic versioning format.

- The components of the tag shall be incremented both in case of an update to TSF documentation, and in case of updating to a new release of the upstream nlohmann/json library.
- Updates to TSF documentation shall increment the PATCH or MINOR component.
- Updating the version of nlohmann/json to a new release shall increment the appropriate tag component based on the extensiveness and nature of the upstream changes.
- To indicate the version of nlohmann/json in use, the nlohmann/json release tag shall always be clearly included in the release notes of this repository.
- The release notes of this repository shall always indicate whether the release includes changes to only TSF documentation, only the version of nlohmann/json, or both.

To update either the version of nlohmann/json within S-CORE or TSF documentation, please refer to the respective Update Concepts below.

# Update Concept for the version of nlohmann/json within S-CORE

## Assumptions of use

This description of the update process is based on the following structure of the repository.
It is assumed that the repository has a default branch called ``main`` containing the most recent documented version of ``nlohmann/json`` together with its documentation.
The releases of the documented nlohmann/json version are identified by release notes on ``main``.

Note that there is **no automatic information** on the existence of a new release in the original ``nlohmann/json``; instead the possibility to update is detected **manually**.
Note further that, due to the currently relatively limited use of nlohmann/json within S-CORE, there appears currently no inherent need to keep the version up to date.

## Update process of the original nlohmann/json

The releases of ``nlohmann/json`` are collected on the [Release site](https://github.com/nlohmann/json/releases) of the repository ``nlohmann/json``.
Each release announcement is expected to contain the release date, SHA-256 values for json.hpp, include.zip and json.tar.xz, and a brief list containing bug fixes, improvements, further changes and deprecated functions.
The new release is expected to be located within the branch **master** in nlohmann/json, from where the most recent version can be drawn.

## Update process of the S-CORE version

In the following, we shall describe the intricacies of updating the version of ``nlohmann/json`` within Eclipse S-CORE.
This version is not a mere fork of the original master branch of ``nlohmann/json``, but instead enriched with the documentation following the Trustable Software Framework (TSF).

The enrichment with the documentation necessitates some changes to the fork of the original repository.
For the most part, these changes are in-obtrusive, and mere additions.
In particular, the folders ``include`` and ``single-include`` remain unchanged, and should be updated without further adaptations.
In some cases, however, additional tests are run and data are generated and collected, which were not run or generated in the original ``nlohmann/json``, so that obtrusive changes of files were necessary.
For these files, and in particular the workflow files, caution must be exercised, as to not disturb the documentation.
Moreover, some parts of the documentation must be adapted to the new version.


### What can not be updated without further precautions?

* ``cmake/ci.cmake``
    This file defines, in particular, the various custom cmake targets; in particular, the various configurations for the execution of the unit- and integration-tests are defined.
    The TSF requires, or, at the very least, strongly encourages us to collect test-results.
    In order to do this efficiently, the ctest command is adapted to automatically generate the junit-logs of each test-run.
    For this, the option ``--output-junit`` is set with output path ``../my_logs/TARGETNAME_junit.xml``, where TARGETNAME is replaced by the name of the respective cmake target; in case that this convention is insufficient to uniquely identify the logs, TARGETNAME is amended by a number.
    When updating, it must be ensured that these adaptations are preserved.
    Moreover, if the update introduces new cmake targets or new executions of ctest, it must be ensured, that the junit-log is generated and stored with a similar naming convention in the folder "../my_logs/".
    Otherwise, it can not be ensured that the test data are accurately captured.

* ``cmake/download_test_data.cmake``
    This file is modified to ensure that the test-data are not downloaded from the original test-data repository, but instead from the copy of that repository within the Eclipse S-CORE organisation.
    It must be ensured that this change is preserved.

* ``tests/CMakeLists.txt``
    This file collects, in particular, the files containing the unit- and integration-tests in a list, which is given to cmake. 
    Custom tests were added in TSF/tests to document the fulfillment of the expectations. 
    To ensure that these tests are run, the file tests/CMakeLists.txt has been modified.
    During the update, it must be ensured, that the custom tests are still being executed.

* ``.github/workflows/parent-workflow.yml``
    To ensure a specific execution order for the individual github workflows, their execution is orchestrated by the parent-workflow.
    To guarantee that this order is respected, it must be ensured that every workflow except for ``comment_check_amalgamation.yml``, ``docs-cleanup.yml``, ``parent-workflow.yml``, ``scorecards.yml`` and ``stale.yml`` runs ``on workflow_call``, only.

* ``.github/workflows/ubuntu.yml``
    The ubuntu workflow orchestrates the parallel execution of various cmake targets with varying configurations running on the latest version of ubuntu.
    The first TSF related adaptation of this workflow is that every step, in which a junit-report is generated, generates an artifact.
    It must be ensured, that these artifacts are still generated after the update.
    The second adaptation is that the test-results are captured, processed and persistently stored or stored in the ubuntu-artifact.
    Therefore, it must be ensured that the jobs ``publish_test_data_success``, ``publish_test_data_failure``, ``publish_test_data_cancellation`` and ``ubuntu_artifact`` are executed.
    Moreover, in case that any further job is added by nlohmann, it must be ensured that this job is added to the list of jobs required before the latter workflows are executed.
    If any further job added by nlohmann generates a junit-log, it must be ensured that this job generates an artifact containing its junit-logs. 

* ``.github/workflows/cifuzz.yml``
    This workflow uses Google's oss-fuzz, which is not available to the copy within Eclipse S-CORE. 
    Therefore, this workflow needs to be disabled in the copy. 
    Currently, this is done by removing it altogether, which we recommend to do so that no confusion as to why this workflow is not executed arises. 

* ``.github/workflows/publish_documentation.yml``
    The original version of this workflow is replaced with a completely customised version, which reflects the use of trudag and the integration into the Eclipse S-CORE organisation.
    Therefore, it is recommended to not change this workflow. 
    In particular, the version of publish_documentation.yml in the original repository nlohmann/json must not replace the publish_documentation.yml of the present repository.

* ``.github/workflows/test_trudag_extensions.yml``
    This workflow is not present in the original nlohmann/json and must not be removed, or modified (besides updating the versions of tools, if necessary) by the update.

* Other entries of ``.github/workflows``
    For every workflow, it must be ensured that the conditions of their execution are unchanged.
    The workflows ``check_amalgamation``, ``codeql``, ``dependency_review``, ``labeler`` and ``test_trudag_extensions`` generate an artifact, which must not be changed.
    New workflows should be carefully reviewed.
    If it is determined that their execution within the project is beneficial, and that they do not interfere with, then they should be integrated within the parent workflow at an appropriate place and their execution condition should be set to on ``workflow``, or their execution should be scheduled appropriately. 
    It is strongly recommended that the new workflow produces an artifact on success, and that the validator ``check_artifact_exists`` is adapted accordingly.
    If nlohmann deletes any of the currently executed workflows, in particular ``check_amalgamation.yml``, ``codeql.yml``, ``dependency_review.yml``, ``labeler.yml``, ``test_trudag_extensions.yml`` and ``ubuntu.yml``, then it is strongly recommended to keep the currently executed version, since the automatic validator ``check_artifact_exists`` depends on the existence of these workflows.
    In case that it is determined that these workflows should be deleted also in the documented copy of ``nlohmann/json``, then the validator ``check_artifact_exists`` and all its occurrences must be adapted accordingly.

* ``ChangeLog.md``
    It must be ensured that the changes of the new release of nlohmann/json are properly described in the file ``ChangeLog.md``.


### Necessary adaptations

The following adaptation is recommended, and has, unfortunately, not been automated.

* ``TSF/trustable/statements/JLS-02.md``
    It must be carefully ensured that this statement and its references are still valid. In particular, it is strongly recommended to refer to a fuzz testing result running on the version that is updated to.


The following adaptations to the documentation have been automated; the python-script TSF/scripts/update_helper.py may be used to assist with these changes. 
For the error-free execution is it necessary, however, to adhere to the naming scheme json_version_X_XX_X, and to not change the structure of the directories.

* ``TSF/Trustable/statements/JLS-11.md``
    It must be ensured that the correct release date is used.

* ``TSF/trustable/statements/JLS-14.md``
    It must be ensured that the release of the correct version is referenced.
    Furthermore, the sha-value of the evidence must be adapted to the one provided in that announcement post.

* ``TSF/trustable/docs/introduction/index.rst``
    In this file, the version of ``nlohmann/json`` that is documented is explicitly mentioned at two places. 
    This version must be updated.
    
* ``TSF/scripts/generate_list_of_misbehaviours.py``
    This script contains version and release date hard-coded. Both must be updated.


### Recommended procedure

Based on the above observations, the following steps are recommended for each update to the library.

1. Merge branch master from the original nlohmann/json into an external fork of the eclipse-score/inc_nlohmann_json repository, where steps 2-12 shall be performed.
2. Confirm the deletion of cifuzz.yml, macos.yml and windows.yml.
3. Resolve the potential merge conflict in publish-documentation.yml by rejecting the incoming changes.
4. Update the versions of the github actions, if necessary. 
5. Resolve the potential merge conflicts in check_amalgamation.yml, codeql.yml, dependency_review.yml, labeler.yml and test_trudag_extensions.yml to ensure that the artifacts are generated, i.e. the jobs ``Generate XXX artifact`` and ``Upload XXX artifact`` are retained.
6. Resolve the potential merge conflict in ubuntu.yml following the above instructions.
7. Resolve the potential merge conflicts in cmake/download_test_data.cmake and cmake/ci.cmake following the above instructions.
8. Carefully examine the automatically merged changes. If no interference is to be expected, complete the merge.
9. In case any additional workflow has been added, carefully examine and integrate into the parent-workflow or schedule appropriately.
10. Adapt the documentation as described above.
11. Generate the TSF report locally and carefully investigate any change in the trustable score(s). 
    If any relevant behaviour of the library changes, adapt the documentation. 
    Additionally, if any additional tests were added, or existing tests were changed, carefully investigate whether these warrant an amendment of the documentation.
12. Merge into the ``main`` branch of eclipse-score/inc_nlohmann_json.
13. Create a new release tag in eclipse-score/inc_nlohmann_json, following semantic versioning.

# Update concept for the TSF documentation

## Assumptions of use

The documentation follows the Trustable Software Framework (TSF), which is documented [here](https://codethinklabs.gitlab.io/trustable/trustable/print_page.html).
Furthermore, the automatic generation of the TSF report and the tracking of changes to the core functionalities of the library uses _trudag_, which is developed by Codethink and located [here](https://gitlab.com/CodethinkLabs/trustable/trustable).


## Version of trudag

The documentation is currently built using trudag version v2025.10.22
In case of new releases of trudag in the future, it is recommended to carefully review the introduced changes and rigorously test it before merging it into eclipse-score/inc_nlohmann_json.
The following should be considered:

* How has the algorithm for the accumulation of the trustable score changed? Ideally, it is not changed, otherwise the necessity for a new review arises.
* How has the data store interface changed? Ideally, it is not changed, but if it does and the expected schema format changes, data_store.py needs to be updated accordingly.
* How has the the expected configuration for the TSF items changed? It is known that this configuration changed (at least) once before. What does the potential change mean?
* Do all custom references and validators as well as the data store interface work as before?
* Has the algorithm for the hashing changed, or are there any changes to the trustable scores? If so, investigate carefully!


## Subject-Matter-Expert-scores

The intention with the SME scores is to find the _true_ trustable score by means of a heuristic law-of-large-numbers argument. 
Therefore, it is very much welcome for contributors to add their SME scores to statements for which they feel confident to do so.
While the committer may check SME scores for plausibility, it is recommended to not question SME scores as this interferes with the assumed independence of the SME!
It is highly recommended to not delete SME scores under usual circumstances; most certainly, the SME scores should never be changed by anybody except the original SME.
The following unusual circumstances can, after careful consideration, justify the removal or (much preferably!) the request for re-evaluation by the original SMEs:

* change of references: 
    If, e.g. due to an update of ``nlohmann/json``, the references of any items (be it tests or code) changes, then this should trigger a re-evaluation of the statement. 
    In particular if the behaviour changed significantly, it can be justifiable to assume that the old SME scores do not reflect the statement anymore.
* addition of automatic validators: 
    Recall that the SME-scores have different meanings depending on whether or not an automatic validator is implemented. 
    In the absence of a validator, the SME shall assess their confidence in the statement based on linked artifacts (references) and their own knowledge. 
    In the presence of a validator, the SME shall assess only their confidence in the validator as an accurate measure of the truth of the statement.

Some important remarks regarding the process of performing an SME review include:

* Assumptions-of-Use (AoU) items are not to be scored by an SME. These should at some point in time be converted to a regular statement by the integrator, but shall until then remain without SME scores, evidence or references. These automatically contribute with a score of 0 for the time being.
* Any statement that has supporting statements (child nodes) shall not be scored by an SME. The scores for these shall only comprise of the automatic calculation of the mean of the scores of all linked child nodes.
* The TSF report is only published for the default branch of a repository. In order to to see TSF documentation for a non-default branch, the reviewer can checkout the branch in a clone and locally generate the report by running the ``TSF/scripts/generate_documentation.sh`` script.
* Any TSF item that has the ``normative`` parameter set to false shall not be scored by the SME. These are just informational and do not make a statement about the project. Note that the parameter itself shall not be set or changed by the SME, but is defined by the contributor who created the TSF item.

For documentation on how to actually perform a SME review, please refer to the following:
* https://codethinklabs.gitlab.io/trustable/trustable/trudag/usage.html#reviewing-items 
* https://codethinklabs.gitlab.io/trustable/trustable/trudag/usage.html#sme-review

## Validators

The automatic validators are intended to calculate a trustable score based on quantifiable data. 
In particular the introduction of a validator changes the meaning of the (potential) SME scores associated to a statement.
Therefore, the change or introduction of an automatic validator is most critical.
In case the validator used in a statement has been changed, it is highly recommended to urge the original SME to re-review the statement and adapt their scores, or (at the very least) to enlist additional SMEs to judge the changed statement.
After careful consideration the highly critical decision to remove some SME scores no longer reflecting the statement could be made. 

## References

References should be treated in the same way as validators, i.e. any update of a reference should trigger a re-review by the SME.
For references, however, the decision to remove a stale SME score is even more critical unless the reference reveals critical new information, which is highly unlikely, or the change of the reference is triggered by a significant change in the behaviour of the library, which heavily affected the statement.